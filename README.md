# Real-Time Hand Gesture Controlled Media Player on Jetson Nano

An edge-AI based real-time gesture recognition system that enables touchless media control using NVIDIA Jetson Nano.

The system detects hand landmarks using MediaPipe, extracts normalized features, classifies gestures using a trained ML model (Random Forest / MLP), and controls media playback via MPV using IPC socket communication.

---

# Table of Contents

- Project Overview
- System Architecture
- Features
- Gesture Mapping
- Dataset Collection
- Model Training
- Deployment on Jetson Nano
- Performance Metrics
- Technical Design Decisions
- Installation Guide
- Running the System
- Future Improvements
- Author

---

# Project Overview

This project demonstrates a complete edge AI pipeline:

Camera Input â†’ MediaPipe Landmark Extraction â†’ Feature Normalization â†’ ML Classification â†’ Media Player Control

The model is trained offline on a laptop and deployed on Jetson Nano for real-time inference.

The system operates fully offline and does not require internet connectivity.

---

# System Architecture
Intel RealSense Camera
â†“
MediaPipe Hand Landmark Detection (21 keypoints)
â†“
Wrist-relative Normalization
â†“
Feature Vector (63 values)
â†“
ML Model (Random Forest / MLP)
â†“
Gesture Prediction
â†“
MPV IPC Socket Command
â†“
Media Control


---

# âœ¨ Key Features

- Real-time gesture detection
- Edge deployment (Jetson Nano)
- Lightweight ML inference
- Gesture stabilization & debouncing
- FPS and latency overlay
- Hardware-accelerated media playback
- IPC-based MPV control (robust & efficient)

---

# ðŸŽ® Supported Gestures

| Gesture        | Action              |
|---------------|--------------------|
| Palm          | Play               |
| Fist          | Pause              |
| ThumbUp       | Volume +           |
| ThumbDown     | Volume âˆ’           |
| FastForward   | Seek +10 seconds   |
| FastBackward  | Seek âˆ’10 seconds   |

---

# Dataset Collection

Dataset is collected manually using MediaPipe hand landmarks.

Each sample contains:

- 21 hand landmarks
- (x, y, z) coordinates
- Wrist-relative normalization
- Scale normalization using hand size
- Gesture label
- User ID

Total Features per sample:
21 landmarks Ã— 3 coordinates = 63 features

Dataset stored as: gesture_dataset_cleaned.csv


---

# Model Training

Training is performed offline on laptop for computational efficiency.

### Models Used:

- Random Forest Classifier
- Multi-Layer Perceptron (MLP)

### Training Pipeline:

1. Load dataset
2. Remove unnecessary columns
3. Split into train/test sets
4. Apply scaling (for MLP)
5. Train model
6. Evaluate accuracy
7. Save model using joblib

### Example:

```bash
python train_model_jetson.py  

